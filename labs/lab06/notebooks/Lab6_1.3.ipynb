{
  "metadata": {
    "name": "Lab6",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sc"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# WORDCOUNT COMPACTO\n\nfiles_rdd \u003d sc.textFile(\"s3://lab6-sgg/books_dataset/epubtxt/*.txt\")\n\nwc_unsort \u003d files_rdd.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\nwc \u003d wc_unsort.sortBy(lambda a: -a[1])\nfor tupla in wc.take(10):\n        print(tupla)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfiles_rdd \u003d sc.textFile(\"s3://lab6-sgg/books_dataset/epubtxt/*.txt\")\nfor f in files_rdd.take(10):\n    print(f)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntokens \u003d files_rdd.flatMap(lambda line: line.split())\nfor t in tokens.take(10):\n    print(t)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nwc1 \u003d tokens.map(lambda word: (word, 1))\nfor c in wc1.take(10):\n    print(c)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nwc \u003d wc1.reduceByKey(lambda a, b: a + b)\nfor c in wc.take(10):\n    print(c)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nwcsort \u003d wc.sortBy(lambda a: -a[1])\nfor c in wcsort.take(10):\n    print(c)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nwc.coalesce(1).saveAsTextFile(\"s3://lab6-sgg/emr_output/zeppelin_notebook\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}